1) A Java program, to take a HDFS Path as input and display all the files and sub-directories
in that HDFS path.

import java.io.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FileStatus;

public class FileListing {
	public static void main(String[] args) {
		if (args.length != 1) {
			System.out.println("Pass one argument");
			System.exit(1);
		}
		
		Path path = new Path(args[0]);
		
		try
		{
			Configuration conf = new Configuration();
			FileSystem fileSystem = FileSystem.get(path.toUri(), conf);
			FileStatus[] fileStatus=fileSystem.listStatus(path);
			
			for (FileStatus fStat : fileStatus) 
      {
            System.out.println(fStat.getPath());
			}
			}

		}
		catch (IOException e)
		{
            e.printStackTrace();
		}
	}
}

2)Modify the previous program to list all the files and sub-directories in the HDFS path
recursively.

import java.io.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.RemoteIterator;
import org.apache.hadoop.fs.FileStatus;

public class FileListing {
	public static void main(String[] args) {
		if (args.length != 1) {
			System.out.println("enter anyone argument");
			System.exit(1);
		}
		
		Path p = new Path(args[0]);
		
		try
		{
			Configuration c = new Configuration();
			FileSystem fileSystem = FileSystem.get(path.toUri(), c);
			FileStatus[] fileStatus=fileSystem.listStatus(p);
			RemoteIterator<LocatedFileStatus> it=filesystem.listFiles(p,true);
			while(it.hasNext())
			{
				System.out.println(it.next().getPath());
			}

		}
		catch (IOException e)
		{
            e.printStackTrace();
		}
	}
}








